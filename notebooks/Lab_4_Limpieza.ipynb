{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64517fc2",
   "metadata": {},
   "source": [
    "# Procedimientos de Limpieza de Comentarios de Facebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d71f0f",
   "metadata": {},
   "source": [
    "## Cargar el archivo\n",
    "Se extrajeron 1,569 comentarios de septiembre y octubre año 2020 en los que se menciona a CitiBanamex\n",
    "\n",
    "Instalamos la librería mlxtend que nos permite implementar reglas de asociación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68915d24-e5ba-4aba-9917-d50fd9412b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883e6c27-abbc-44b0-a648-65a29993a4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c conda-forge spacy\n",
    "# python -m spacy download es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44286594",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/BanamexFace.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a1d537-f293-4bf3-8804-e914e5827224",
   "metadata": {},
   "source": [
    "## Limpieza de Texto\n",
    "#### Eliminamos los signos de Puntuación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6d638f-3366-43b6-ba9e-05225c8e0b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(text):\n",
    "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return text\n",
    "\n",
    "df['com_p'] = df['com'].apply(lambda x: remove_punct(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ded9d7",
   "metadata": {},
   "source": [
    "#### Creamos una columna con los comentarios tokenizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb37031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = re.split('\\W+', text)\n",
    "    return tokens\n",
    "df['com_t'] = df['com_p'].apply(lambda x: tokenize(x.lower()))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092b303e-d854-439f-b8f1-53e15be6dee7",
   "metadata": {},
   "source": [
    "## POS Tagging\n",
    "#### Invocamos al POS Tagger de Stanford (https://nlp.stanford.edu/software/spanish-faq.shtml#tagset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cfbd83-f115-4513-8f44-9287d78336a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.internals import find_jars_within_path\n",
    "from nltk.tag import StanfordPOSTagger\n",
    "%env JAVAHOME=C:\\Program Files\\Java\\jre1.8.0_321\n",
    "tagger=\"C:\\\\Users\\\\lc250058\\\\notebooks\\\\Texto\\\\src\\\\models\\\\spanish-ud.tagger\"\n",
    "jar=\"C:\\\\Users\\\\lc250058\\\\notebooks\\\\Texto\\\\src\\\\stanford-postagger.jar\"\n",
    "etiquetador=StanfordPOSTagger(tagger,jar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019e6dbc-9c93-4744-9bc7-3ff218853b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "etiquetas=etiquetador.tag(df.iloc[0,9])\n",
    "for etiqueta in etiquetas:\n",
    "    print(etiqueta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ed9d00-f62b-4d2d-a3fd-94ba18a4657c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos = pd.DataFrame(columns = ['comm_id', 'term', 'tag'])\n",
    "for i in range(20):\n",
    "    etiquetas=etiquetador.tag(df.iloc[i,9])\n",
    "    ori = [etiqueta[0] for etiqueta in etiquetas]\n",
    "    lis = [etiqueta[1] for etiqueta in etiquetas]\n",
    "    for j in range(len(lis)):\n",
    "        try:\n",
    "            cid = i\n",
    "            txt = ori[j]\n",
    "            new = lis[j]\n",
    "            df_pos = df_pos.append({'comm_id' : cid, 'term' : txt, 'tag' : new}, ignore_index = True)\n",
    "        except Exception:\n",
    "            pass\n",
    "df_pos.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395316c9-9c85-4ae2-884a-0fd8173623cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbf8154-0dd7-4ac0-a35a-2abec5d4c999",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "#### Stemming en Español con SnowBall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd05303d-4d84-49e1-87ee-d718dbc41a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "stm = SnowballStemmer('spanish') # Hay que indicarle el idioma\n",
    "[stm.stem(word) for word in df.iloc[0,9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74bdf1f-51c4-4636-b3f8-369ab79fc81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stem = pd.DataFrame(columns = ['comm_id', 'stem'])\n",
    "for i in range(len(df)):\n",
    "    lis = [stm.stem(word) for word in df.iloc[i,9]]\n",
    "    for tok in lis:\n",
    "        try:\n",
    "            cid = i\n",
    "            new = tok\n",
    "            df_stem = df_stem.append({'comm_id' : cid, 'stem' : new}, ignore_index = True)\n",
    "        except Exception:\n",
    "            pass\n",
    "df_stem.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a0478e-cce8-4f00-b211-7ecd5f433a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stem.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7abb73a-b83d-4fef-af7d-56c55192f2af",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "#### Lemmatization con WordNet (Sólo inglés)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e760d37-de57-4e3e-8680-a5e7bf1a511e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemm = WordNetLemmatizer()\n",
    "[lemm.lemmatize(word) for word in ['cars','dogs','families','parties','elements']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e147a4e-4d19-424c-9767-0771ae7f5794",
   "metadata": {},
   "source": [
    "#### Lemmatization con spacy (Utilizando un Corpus en Español)\n",
    "Es necesario instalar spacy y el modelo de lenguaje en español:\n",
    "```\n",
    "python -m spacy download es\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c6a891-5ccf-43db-a44f-cae62bb5d576",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "doc = nlp(df.iloc[0,8])\n",
    "[tok.lemma_ for tok in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3a448e-ee68-46ce-9532-2eacbd8d4eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lem = pd.DataFrame(columns = ['comm_id', 'lemma'])\n",
    "for i in range(len(df)):\n",
    "    doc = nlp(df.iloc[i,8])\n",
    "    lis = [term.lemma_ for term in doc]\n",
    "    for tok in lis:\n",
    "        try:\n",
    "            cid = i\n",
    "            new = tok\n",
    "            df_lem = df_lem.append({'comm_id' : cid, 'lemma' : new}, ignore_index = True)\n",
    "        except Exception:\n",
    "            pass\n",
    "df_lem.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713c869b-22eb-401a-8bd3-11373ab016ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lem.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386cf2c0-1289-4644-b5a8-8820c28ca1cc",
   "metadata": {},
   "source": [
    "## N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab82482e-7d42-40a5-a837-90cadc8fde3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "words = df.iloc[0,9]\n",
    "num_elementos = 3\n",
    "n_grams = ngrams(words, num_elementos)\n",
    "for grams in n_grams:\n",
    "    print (grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d13827f-52a2-49ef-af3e-155e8d9c90b7",
   "metadata": {},
   "source": [
    "## Corrección Ortográfica\n",
    "Utilizamos el código basado en diccionario, creado por Peter Norvig: http://www.norvig.com/spell-correct.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75912cc-5e7a-411d-a20f-91079565365b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run C:\\Users\\lc250058\\notebooks\\Texto\\src\\spell.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b31ecf-13ad-44f4-9dc8-ac0a42c67789",
   "metadata": {},
   "outputs": [],
   "source": [
    "correction('natufaleza')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090537ec-0e49-446a-9159-991b0ba7364f",
   "metadata": {},
   "outputs": [],
   "source": [
    "[correction(term) for term in df.iloc[1,9]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037bd856-5eb7-4dcb-8bbe-1c9c29efbdf6",
   "metadata": {},
   "source": [
    "## Reconocimiento de Entidades (NER)\n",
    "Utilizamos el mismo corpus de spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4ca6d1-0778-4a87-bedb-4b493c9e160c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc349c7-c098-4ede-80fe-5d0b076918be",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(df.iloc[0,8])\n",
    "for sent in doc.sents:\n",
    "    displacy.render(nlp(sent.text),style='ent',jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3464b95-0f2b-4372-b1bb-4eae6d75c72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    doc = nlp(df.iloc[i,8])\n",
    "    for sent in doc.sents:\n",
    "        displacy.render(nlp(sent.text),style='ent',jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bae14b6",
   "metadata": {},
   "source": [
    "Elaborado por Luis Cajachahua bajo licencia MIT (2022)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
